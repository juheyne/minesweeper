\chapter{Deep-Reinforcement Learning}
Our first attempt to solve the problem of a neural network playing minesweeper is with deep--reinforcement learning.
Techniques able to learn Atari games~\cite{mnih2013playing} with a high input complexity should be able to learn and play `simpler' games like Minesweeper.
First we introduce the concept of Reinforcement learning with the addition of Deep Reinforcement Learning.
Afterwards we present our approach, followed by our results and the problems we encountered.

\section{Background}
Reinforcement learning is used to learn an optimal policy for an agent in an environment by exploring the environment and getting rewards based on its performance.
The environment $\mathcal{E}$ can be regarded as a sequence of actions, states and rewards.
At every time--step the agent selects a legal action $a_t$ from a set of legal game actions. 
The goal of the agent is to maximize the future reward.
We use as the future reward, the standard future discounted reward
\begin{equation}
R_t := \sum_{t'=t}^T \gamma^{t'-t}r_{t'},
\end{equation}
where $\gamma$ is a discount factor per time--step, $r_t$ represents the change in the game score and $T$ is the time--step at which the game is terminated.
The optimal action--value function $Q^*\left(s,a\right)$ is defined as the maximum expected return that will be returned by following any strategy:
\begin{equation}
Q^*\left(s,a\right):= \max_\pi \mathbb{E} \left[ R_t|s_t=s\text{, }a_t=a\text{, } \pi\right]
\end{equation}
where $s$ is some sequence and $\pi$ is a policy mapping sequences to actions.
%TODO belmann intuition erkl√§ren??
The optimal action--value function is gained with the Bellman equation by maximizing the expected value of $r+\gamma Q^*\left(s',a'\right),$
\begin{equation}
Q^*\left(s,q\right)= \mathbb{E}_{s' \sim \mathcal{E}} \left[ r + \gamma \max_{a'} Q^*\left(s',a'\right)| s,a \right],
\end{equation}
where $Q^*\left(s,a\right)$ is the sequence $s'$ at the next time--step for all possible actions $a'$. 
The value is maximized by selecting the optimal action $a'$.
With the Bellman equation the basic idea of reinforcement learning is introduced with an iterative update:
\begin{equation}
Q_{i+1}\left(s,a\right)= \mathbb{E}\left[r+\gamma \max_{a'} Q_i\left(s,a\right)| s,a\right],
\end{equation} 
where we assume that the value iteration algorithm converges to the optimal action--value function, i.e. $Q_i \rightarrow Q^*\text{, for } i \rightarrow \infty$.

For easy problems with small dimensions for the possible state--action space the Q--function can easily be saved in a table.
This table is updated while learning and represents the Q--function for each possible pair.
The state space for Minesweeper is incredibly large, thus a direct representation as a table does not work.
Neural networks can be used to encode a Q--function in a compact way.

For a neural network we need a non--linear approximation, where $\theta$ are the weights of the Q--network.
The Q--network is trained to minimize a sequence of loss functions $L_i\left(\theta_i\right)$, which changes in every iteration $i$,
\begin{equation}
L_i\left(\theta_i\right)= \mathbb{E}_{s,a \sim \rho \left( \cdot \right)}\left[ \left( y_i - Q\left(s,a;\rho_i \right) \right)^2\right],
\end{equation}
where $yi$ is the target at the $i-th$ iteration,
\begin{equation}
y_i = \mathbb{E}_{s'\sim \mathcal{E}}\left[ r+ \gamma \max_{a'} Q\left( s',a'; \theta_{i-1}\right)| s,a\right],
\end{equation}
and $\rho\left(s,a\right)$ is a probability distribution over the sequences $s$ and actions $a$, that is also called the behavior distribution.
To optimize the loss function $L_i\left(\theta_i\right)$ the parameters from the previous iteration $\theta_{i-1}$ are held fixed. 
Therefore, the target depends on the weights of the network. 
So to optimize we differentiate the loss function with respect to the weights and get the gradient,
\begin{equation}
\nabla_{\theta_i} L_i\left(\theta_i\right)= \mathbb{E}_{s,a\sim\rho\left(\cdot\right);s'\sim \mathcal{E}} \left[ \left( r + \gamma \max_{a'}Q\left(s',a';\theta_{i-1}\right) -Q\left(s,a;\theta_i\right) \right)\nabla_{\theta_i}Q\left(s,a;\theta_i\right)\right].
\end{equation}
Instead of calculating the expectation the loss function is optimized by stochastic gradient descent.
We get the Q--learning algorithm if the weights are updated after every step and the expectation is replaced by samples from $\rho$ and $\mathcal{E}$ respectively.

In contrast to supervised learning the target function depends on the weights of the network.
As the states of a game are highly correlated the basic assumption that the data used for learning is independent of each other does not hold.
Thus a memory is introduced which is filled continuously while playing and sampled randomly for learning.
This removes the correlation between game states as much as possible.
The memory contains the state, its reward, action and the next state.
This reduces overfitting which would occur if the network is directly learned while playing.

As a policy we use $\epsilon$--greedy which takes a random action with the probability $\epsilon$ and otherwise takes the optimal action from the neural network.

\section{Our approach}
The environment is the the minesweeper field represented as an integer array.
An unvisited field is represented through $-1$ and $0$ to $8$ represent the number of adjacent mines.
The number of possible actions is equal to the size of the field and its encoding is the linear index of the two--dimensional array.
For the field size $8 \times 8$ we have 64 possible actions.
If the agent takes action $8$, the field $(1, 0)$ would be opened.

The main problem with Minesweeper compared to for example the Atari games is that the input dimension can change.
A `beginner' game is $8 \times 8$ with $10$ mines, whereas an `advanced' game is $16 \times 16$ with $40$ mines.
This is a big problem as the network needs to be resized and retrained for each size.
For the first try we restrict ourselves to the `beginner' size.

The first thing we noticed is the problem of our input size.
For example the Atari games are $84 \times 84$, whereas our input is $8 \times 8$.
With this small input a `deep' network is just not possible as each layer decreases the input size if you do not add padding.
Padding would create a whole different kind of problem for the encoding.
Thus we could only use a small amount of convolution layers to not destroy any positional information in the input data.
We tried several different architectures, but the results were pretty underwhelming.
The agent was prone to get stuck and it learned to perform worse than a random agent.
The reason for this bad performance is the overestimation of Q--values due to the network.
While a evenly distributed overestimation would not be harmful for the training process, an overestimation in a subset of suboptimal actions results impairs the learning of the optimal policy.
This results in feedback loops leading to a stuck training process, as the suboptimal action is taken over and over again.

To improve the stability of the network during training a separate target network can be used.
This type of Reinforcement Learning is called Double Q--Learning~\cite{HasseltGS15}.
Instead of updating it every few steps, we update $\theta'$, the weights from the target network, slowly every step by trailing the weights $\theta$ from the main network with a factor $\tau$~\cite{lillicrap2015continuous}.
\begin{equation}
	\theta' \leftarrow \tau\theta + (1-\tau)\theta' \text{ with } \tau \ll 1
\end{equation}
This results in a more stable training.
To reduce the overestimating of the Q--values, the estimation of the Q--value is decoupled from the action choice.
The primary network network chooses the action and the Q--value from the target, more stable, network is used.
\begin{equation}
	Q'(s, a) = r + \gamma Q(s', \arg\max_a(Q(s', a, \theta), \theta')) 
\end{equation}
This improves the stability of the training.

\section{Results}


\section{Problems}