\chapter{Deep-Reinforcement Learning}
Our first attempt to solve the problem of a neural network playing minesweeper is with deep--reinforcement learning. 
%TODO cite
had promising results that were tried to achieve for the minesweeper game.
In this chapter we first want to get into the basics of deep--reinforcement learning then we describe how we applied for our setting later we discuss the results and the problems.

\section{Background}
We view the minesweeper grid as a environment $\mathcal{E}$ in which an agent interacts. 
The Environment can be regarded as a sequence of actions, states and rewards.
At every time--step the agent selects an legal action $a_t$ from a set of legal game actions. 
The goal of the agent is to maximize the future reward by selecting actions $a_t$. The future reward
\begin{equation}
R_t := \sum_{t'=t}^T \gamma^{t'-t}r_{t'},
\end{equation}
where $\gamma$ is a discount factor per time--step, $r_t$ represents the chant in the game score and $T$ is the time--step at which the game is terminated.
The optimal action--value function $Q^*\left(s,a\right)$ is defined as the maximum expected return that will be returned by the following strategy:
\begin{equation}
Q^*\left(s,a\right):= \max_\pi \mathbb{E} \left[ R_t|s_t=s\text{, }a_t=a\text{, } \pi\right]
\end{equation}
where $s$ is some sequence and $\pi$ is a policy mapping sequences to actions.
%TODO belmann intuition erkl√§ren??
The optimal action--value function is gained with the Bellman equation by maximizing the expected value of $r+\gamma Q^*\left(s',a'\right),$
\begin{equation}
Q^*\left(s,q\right)= \mathbb{E}_{s' \sim \mathcal{E}} \left[ r + \gamma \max_{a'} Q^*\left(s',a'\right)| s,a \right],
\end{equation}
where $Q^*\left(s,a\right)$ is the sequence $s'$ at the next time--step for all possible actions $a'$. 
The value is maximized by selecting the optimal action $a'$.
With the Bellman equation the basic idea of reinforcement learning is introduced with an iterative update:
\begin{equation}
Q_{i+1}\left(s,a\right)= \mathbb{E}\left[r+\gamma \max_{a'} Q_i\left(s,a\right)| s,a\right],
\end{equation} 
where we assume that the value iteration algorithm converges to the optimal action--value function, i.e. $Q_i \rightarrow Q^*\text{, for } i \rightarrow \infty$.
For a neural network we need a non--linear approximation, where $\theta$ are the weights of the Q--network.
The Q--network is trained to minimize a sequence of loss functions $L_i\left(\theta_i\right)$, which changes in every iteration $i$,
\begin{equation}
L_i\left(\theta_i\right)= \mathbb{E}_{s,a \sim \rho \left( \cdot \right)}\left[ \left( y_i - Q\left(s,a;\rho_i \right) \right)^2\right],
\end{equation}
where $yi$ is the target at the $i-th$ iteration,
\begin{equation}
y_i = \mathbb{E}_{s'\sim \mathcal{E}}\left[ r+ \gamma \max_{a'} Q\left( s',a'; \theta_{i-1}\right)| s,a\right],
\end{equation}
and $\rho\left(s,a\right)$ is a probability distribution over the sequences $s$ and actions$a$, that is also called the behaviour distribution.
To optimize the loss function $L_i\left(\theta_i\right)$ the parameters from the previous iteration $\theta_{i-1}$ are held fix. 
Therefore, the target depends on the weights of the network. 
So to optimize we differentiate the loss function with respect to the weights and get the gradient,
\begin{equation}
\nabla_{\theta_i} L_i\left(\theta_i\right)= \mathbb{E}_{s,a\sim\rho\left(\cdot\right);s'\sim \mathcal{E}} \left[ \left( r + \gamma \max_{a'}Q\left(s',a';\theta_{i-1}\right) -Q\left(s,a;\theta_i\right) \right)\nabla_{\theta_i}Q\left(s,a;\theta_i\right)\right].
\end{equation}
Instead of calculating the expectation the loss function is optimized by stochastic gradient descent. 
We get the Q--learning algorithm if the weights are updated after every step and the expectation is replaced by samples form $\rho$ and $\mathcal{E}$ respectively.

If an agent interacts with an game that has the same start and interacts with the same actions it is very likely, that each time the results will be the same. 
Therefore, we talk about highly correlated games and it is logical to introduce some kind of memory. 
This memory should contain the state, reward, action and the new state. 
One advantage is that the memory can be sampled in batches randomly which can lead to a lower risk of over--fitting.
Though the steps now are taken from the randomized memory rather than from the agents ordered steps through the game.

\section{Our approach}

\section{Results}

\section{Problems}